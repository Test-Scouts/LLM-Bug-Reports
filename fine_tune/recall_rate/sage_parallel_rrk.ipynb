{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import util\n",
    "from collections import Counter\n",
    "from torch import load, Tensor\n",
    "import pandas as pd\n",
    "\n",
    "import multiprocessing\n",
    "import pickle\n",
    "import boto3\n",
    "import math\n",
    "import io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3 = boto3.client('s3')\n",
    "\n",
    "bucket = 'sagemaker-studio-113002098422-fuoot0q3jmu'\n",
    "\n",
    "encodings_file_key = 'thesis/encodings/all-mpnet-base-v2/netbeans/mpnet_netbeans_test_enc.pt'\n",
    "encodings_obj = s3.get_object(Bucket = bucket, Key = encodings_file_key)\n",
    "\n",
    "netbeans_test_file_key = 'thesis/splits/netbeans/netbeans_test.csv'\n",
    "netbeans_relations_file_key = 'thesis/datasets/netbeans/netbeans_pairs - Copia.csv'\n",
    "\n",
    "pickle_file_key = 'thesis/results/all-mpnet-base-v2/mpnet_recall_rate_netbeans_test.pkl'\n",
    "\n",
    "\n",
    "netbeans_test_obj = s3.get_object(Bucket = bucket, Key = netbeans_test_file_key)\n",
    "netbeans_relations_obj = s3.get_object(Bucket = bucket, Key = netbeans_relations_file_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encodings_bytes = encodings_obj['Body'].read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_dict = load(io.BytesIO(encodings_bytes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb = emb_dict['encoded_desc']\n",
    "emb_keys = [x for x in emb.keys()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reports_netbeans_test = pd.read_csv(netbeans_test_obj['Body'], index_col='bug_id')\n",
    "relations_netbeans = pd.read_csv(netbeans_relations_obj['Body'], index_col='issue_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rr_k_dict(prompt: Tensor, tensor_dict: dict, duplicate_ids: set, k: int):\n",
    "\n",
    "    similarity_scores = [] # array that will store tuples with a report id and its similarity score with the prompt\n",
    "\n",
    "    # iterate trough the dataframe\n",
    "    for bug_id in tensor_dict:\n",
    "\n",
    "        # append current report id and cosine similarity for the current report\n",
    "        # and the prompt descriptions the the selected model has generated\n",
    "        try:\n",
    "            similarity_scores.append(\n",
    "                (\n",
    "                    bug_id, \n",
    "                    util.cos_sim(\n",
    "                        prompt,\n",
    "                        tensor_dict[bug_id]\n",
    "                    )\n",
    "                )\n",
    "            )\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    # sort the similarity_scores list based on the similarity scores in descending order\n",
    "    similarity_scores.sort(key=lambda x: -x[1])\n",
    "\n",
    "    relevant_at_top_k = 0 # initialize counter of identified duplicates in top k as 0\n",
    "\n",
    "    # iterate trough the tuples in the similarity_scores array. We skip the first since it will be the prompt itself\n",
    "    for value in similarity_scores[1:k+1]:\n",
    "\n",
    "        # if the current report is a duplicate of the prompt, increase relevant_at_top_k by one\n",
    "        if value[0] in duplicate_ids:\n",
    "            relevant_at_top_k += 1\n",
    "\n",
    "    # the recall rate at k is the number of duplicates retrieved in the first k over the total number of duplicates\n",
    "    recall_rate = relevant_at_top_k / len(duplicate_ids)\n",
    "    return recall_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generalRRK(reports: pd.DataFrame, relations: pd.DataFrame, tensor_dict_keys: list,  tensor_dict: dict, k: int, recall_rate_dict):\n",
    "\n",
    "    for index in tensor_dict_keys:\n",
    "        if not (emb[index] is None):\n",
    "            duplicates_id = []\n",
    "\n",
    "            if index in relations.index:\n",
    "                try:\n",
    "                    duplicates_id_whole_dataset = [int(id) for id in relations.loc[index].values[0].split(';')]\n",
    "                    duplicates_id = [x for x in duplicates_id_whole_dataset if x in reports.index]\n",
    "                except:\n",
    "                    duplicates_id = []\n",
    "\n",
    "            if len(duplicates_id) > 0:\n",
    "                recall_rate = rr_k_dict(emb[index], tensor_dict, set(duplicates_id), k)\n",
    "                recall_rate_dict[index] = recall_rate\n",
    "                # print(f'{index} -> {recall_rate}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_of_chunks = 12\n",
    "chunk_size = math.ceil(len(emb_keys) / num_of_chunks)\n",
    "chunks = []\n",
    "results = []\n",
    "\n",
    "for i in range(0, len(emb_keys), chunk_size):\n",
    "    chunk = emb_keys[i:i + chunk_size]\n",
    "    chunks.append(chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processes = []\n",
    "\n",
    "for chunk in chunks:\n",
    "    manager = multiprocessing.Manager()\n",
    "    result = manager.dict()\n",
    "    results.append(result)\n",
    "\n",
    "    args_process = (reports_netbeans_test, relations_netbeans, chunk, emb, 10, result)\n",
    "    process = multiprocessing.Process(target=generalRRK, args=args_process)\n",
    "\n",
    "    processes.append(process)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for process in processes:\n",
    "    process.start()\n",
    "\n",
    "for process in processes:\n",
    "    process.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_results_counter = Counter({})\n",
    "\n",
    "for result in results:\n",
    "    total_results_counter += Counter(result)\n",
    "\n",
    "total_results_dict = dict(total_results_counter)\n",
    "total_results_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_data = pickle.dumps(total_results_dict)\n",
    "\n",
    "s3.put_object(Body=pickle_data, Bucket=bucket, Key=pickle_file_key"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
