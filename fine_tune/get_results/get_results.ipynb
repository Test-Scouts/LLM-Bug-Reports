{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ec58c775",
   "metadata": {},
   "source": [
    "Install the sentence transformers library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f43b0c9-705a-4ba5-9945-b7f6d0cd33ad",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install -U sentence-transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36b78d6e",
   "metadata": {},
   "source": [
    "Import dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c7d51f4-1545-4cdc-8619-86b5882384a0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sentence_transformers import util\n",
    "from torch import load, Tensor\n",
    "import pandas as pd\n",
    "\n",
    "import multiprocessing\n",
    "import pickle\n",
    "import boto3\n",
    "import math\n",
    "import io"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d4d619d",
   "metadata": {},
   "source": [
    "If the encodings data and data splits are store in a S3 bucket, import and read the data with the boto3 library "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7fd22e5-9a51-4893-a006-bc2b5b5ac7fb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "s3 = boto3.client('s3')\n",
    "\n",
    "bucket = 'bucket name'\n",
    "\n",
    "encodings_file_key = f'encodings file key'\n",
    "encodings_obj = s3.get_object(Bucket = bucket, Key = encodings_file_key)\n",
    "\n",
    "\n",
    "test_encodings_file_key = f'data split file key' # data for the specific dataset split e.g. firefox test split\n",
    "relations_file_key = f'relations file key'       # ground truth file, where the the actual duplicates can be found\n",
    "\n",
    "\n",
    "test_obj = s3.get_object(Bucket = bucket, Key = test_encodings_file_key)\n",
    "relations_obj = s3.get_object(Bucket = bucket, Key = relations_file_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33511e7f-b1bf-429a-8fd9-66d91f69b005",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "encodings_bytes = encodings_obj['Body'].read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a63552bf-60a3-44ab-b25a-8e5661d0e093",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "reports_test = pd.read_csv(test_obj['Body'], index_col='bug_id')\n",
    "relations = pd.read_csv(relations_obj['Body'], index_col='issue_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "668718e4-1b73-47fb-8048-bd6c894d8bbf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "emb_dict = load(io.BytesIO(encodings_bytes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "598107df",
   "metadata": {},
   "outputs": [],
   "source": [
    "emb = emb_dict['encoded_desc']\n",
    "emb_keys = [x for x in emb.keys()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08e47433",
   "metadata": {},
   "source": [
    "If data splits and encodings are stored locally read them from local files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4a392bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_split_file_path = \"data split file path\"\n",
    "# relations_file_path = \"relations file path\"\n",
    "\n",
    "# reports_test = pd.read_csv(data_split_file_path, index_col='bug_id')\n",
    "# relations = pd.read_csv(relations_file_path, index_col='issue_id')\n",
    "\n",
    "# embedding_file_path = \"embeddings file path\"\n",
    "# emb_dict = load(embedding_file_path)\n",
    "# emb = emb_dict['encoded_desc']\n",
    "# emb_keys = [x for x in emb.keys()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19bcc20c",
   "metadata": {},
   "source": [
    "Define functions to gather the model results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed8c37dd-d3d1-4941-b5eb-ffb2b49edf54",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Returns an array with the [true_positives, false_negatives, false_positives, true_negatives] in the top k suggestions that resulted from the model's encodings for a specific report\n",
    "def results(prompt: Tensor, reports: pd.DataFrame, tensor_dict: dict, duplicate_ids: set, k: int):\n",
    "\n",
    "    similarity_scores = [] # array that will store tuples with a report id and its similarity score with the prompt\n",
    "\n",
    "    # iterate trough the dataframe\n",
    "    for bug_id in tensor_dict:\n",
    "\n",
    "        if bug_id in reports.index:\n",
    "\n",
    "            # append current report id and cosine similarity for the current report\n",
    "            # and the prompt descriptions the the selected model has generated\n",
    "            try:\n",
    "                similarity_scores.append(\n",
    "                    (\n",
    "                        bug_id,\n",
    "                        util.cos_sim(\n",
    "                            prompt,\n",
    "                            tensor_dict[bug_id]\n",
    "                        )\n",
    "                    )\n",
    "                )\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "    # sort the similarity_scores list based on the similarity scores in descending order\n",
    "    similarity_scores.sort(key=lambda x: -x[1])\n",
    "\n",
    "    relevant_at_top_k = 0 # initialize counter of identified duplicates in top k as 0\n",
    "\n",
    "    # iterate trough the tuples in the similarity_scores array. We skip the first since it will be the prompt itself\n",
    "    for value in similarity_scores[1:k+1]:\n",
    "        # if the current report is a duplicate of the prompt, increase relevant_at_top_k by one\n",
    "        if value[0] in duplicate_ids:\n",
    "            relevant_at_top_k += 1\n",
    "\n",
    "    positives = len(duplicate_ids)\n",
    "    negatives = len(reports) - len(duplicate_ids)\n",
    "\n",
    "    false_positives = k - relevant_at_top_k\n",
    "    false_negatives = len(duplicate_ids) - relevant_at_top_k\n",
    "\n",
    "    true_positives = relevant_at_top_k\n",
    "    true_negatives = negatives - false_positives\n",
    "\n",
    "    return [true_positives, false_negatives, false_positives, true_negatives]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83316720-ce99-418a-a4db-7aa4ed6a0205",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Run the function above for each report to collect their results\n",
    "def generalResults(reports: pd.DataFrame, relations: pd.DataFrame, tensor_dict_keys: list,  tensor_dict: dict, k: int, confusion_matrix_dict):\n",
    "\n",
    "    # for each report encoded\n",
    "    for index in tensor_dict_keys:\n",
    "        if not (tensor_dict[index] is None):\n",
    "            duplicates_id = []\n",
    "\n",
    "            # filter the report's duplicates to be the one's in the current split\n",
    "            if index in relations.index:\n",
    "                try:\n",
    "                    duplicates_id_whole_dataset = [int(id) for id in relations.loc[index].values[0].split(';')]\n",
    "                    duplicates_id = [x for x in duplicates_id_whole_dataset if x in reports.index]\n",
    "                except:\n",
    "                    duplicates_id = []\n",
    "\n",
    "            if len(duplicates_id) > 0:\n",
    "                # Get the results for the current report and save them in the confusion_matrix_dict datastructure\n",
    "                confusion_matrix = results(tensor_dict[index], reports, tensor_dict, set(duplicates_id), k)\n",
    "                confusion_matrix_dict[index] = confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20ad8734",
   "metadata": {},
   "source": [
    "Due to the splits large size, it takes a long time to get the results for all the reports. To speed up the process we can use multiple processes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "488899a1-eafd-40d6-8da6-fcf777647596",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "num_of_chunks = 33 # the number of process you can use will vary depending on your cpu\n",
    "# split the reports into chunks to be sent to a process (here we just split the encodings that are used as prompts, they will still be compared to all the reports in the split)\n",
    "chunk_size = math.ceil(len(emb_keys) / num_of_chunks)\n",
    "chunks = []\n",
    "results = []\n",
    "\n",
    "for i in range(0, len(emb_keys), chunk_size):\n",
    "    chunk = emb_keys[i:i + chunk_size]\n",
    "    chunks.append(chunk)\n",
    "\n",
    "print(len(chunks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fda289af-8bc1-40c2-a396-a0803dbed7cf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# create the processes\n",
    "processes = []\n",
    "k = 5\n",
    "\n",
    "for chunk in chunks:\n",
    "    manager = multiprocessing.Manager()\n",
    "    result = manager.dict()\n",
    "    results.append(result)\n",
    "\n",
    "    args_process = (reports_test, relations, chunk, emb, k, result)\n",
    "    process = multiprocessing.Process(target=generalResults, args=args_process)\n",
    "\n",
    "    processes.append(process)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "931f6ebf-a8c0-4771-89a1-6c4334a05c34",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# start the processes\n",
    "for process in processes:\n",
    "    process.start()\n",
    "\n",
    "# wait for all process to finish\n",
    "for process in processes:\n",
    "    process.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34c61d9b-a4c9-4e6f-bbb0-63b0cae7edab",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# merge all the results from each process\n",
    "total_results_dict = {}\n",
    "for result in results:\n",
    "    test = dict(result)\n",
    "    total_results_dict |= test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06f18382",
   "metadata": {},
   "source": [
    "With the results with the [true_positives, false_negatives, false_positives, true_negatives] format, we can calculate multiple metrics, such as recall rate@k and precision@k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f90c140d-41d1-48f6-8359-28d390209b16",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "recall_values = {}\n",
    "precision_values = {}\n",
    "\n",
    "total_true_positives = 0\n",
    "total_false_negatives = 0\n",
    "\n",
    "for key, value in total_results_dict.items():\n",
    "    true_positives = value[0]\n",
    "    false_negatives = value[1]\n",
    "    false_positives = value[2]\n",
    "\n",
    "    #recall rate at k is the number of actual duplicate report in the top k suggestions (true positives) devided by all the actual duplicate report a bug has (true positives + false negatives)\n",
    "    recall = true_positives / (true_positives + false_negatives)\n",
    "    #precision at k is the number of actual duplicate report in the top k suggestions (true positives) devided by k\n",
    "    precision = true_positives / k\n",
    "    \n",
    "    recall_values[key] = recall\n",
    "    precision_values[key] = precision\n",
    "    \n",
    "    total_true_positives += true_positives\n",
    "    total_false_negatives += false_negatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b77fb379-1267-419c-ad01-16fd6eeb1e6e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(total_true_positives)\n",
    "print(total_false_negatives)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71809dae",
   "metadata": {},
   "source": [
    "We can use the pandas library to form data series and get statistics such as mean, median, standard deviation ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0ad9034-0a0f-4fcc-bbe7-3f17967c6cbc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "precision_series = pd.Series(precision_values)\n",
    "precision_series.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce163b02-c952-4dd4-9de7-9e50d5fcbe90",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "rr_series = pd.Series(recall_values)\n",
    "rr_series.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f9b17c2",
   "metadata": {},
   "source": [
    "Save the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d88cce48-a9cf-48ef-bb56-3905a9d75a41",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pickle_file_key = 'file path'\n",
    "pickle_data = pickle.dumps(total_results_dict)\n",
    "\n",
    "s3.put_object(Body=pickle_data, Bucket=bucket, Key=pickle_file_key)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
